{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#This is a code reading comprehension practice.\n",
    "## Below there is a block of code below written in Jax/Haiku.<br>\n",
    "This code describes a [Transformer model](https://arxiv.org/abs/1706.03762). The model is intended to train to predict the lipophilicity (logD) directly from a SMILES string of a molecule. However there are errors in this code.<br>\n",
    "\n",
    "\n",
    "## We would like you to answer a few questions about the code.\n",
    "Please answer each questions in less than 5 sentences. <br>\n",
    "\n",
    "## We would also like you to try to give a corrected version of the model\n",
    "\n",
    "Note 1. There is no dataset and training loop. **You don't need to train the model.** <br>\n",
    "Note 2. The Jax/Haiku code reads very similar to PyTorch or Tensorflow. <br>\n",
    "Don't worry about the exact syntax when writing the corrected version of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task and dataset explanation:\n",
    "We have a simple artificial dataset where each data point is a pair of (SMILES, logD). In this artificial dataset you can assume all SMILES strings have 10 characters and that logD is a real number which is the regression target.\n",
    "\n",
    "You can find some example data in the code block below.\n",
    "Note that we assume the characters in SMILES are tokenized (str->List[int]) already.\n",
    "\n",
    "[**SMILES**](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) <br>\n",
    "The simplified molecular-input line-entry system (SMILES) is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings.\n",
    "\n",
    "For example, ethanol ($C_2H_6O$, 2D drawing shown below) can be written as **CCO** in the SMILES string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two example data points here:\n",
      "--------------------------------------------------\n",
      "c1ccccc1NO      logD: 1.48               \n",
      "Oc1ccccc1N      logD: 0.97               \n",
      "\n",
      "The tokenized data shown below:\n",
      "the minibatch of size = 2:\n",
      "--------------------------------------------------\n",
      "inputs:  [2 1 2 2 2 2 2 1 3 4] logD:  1.48\n",
      "inputs:  [4 2 1 2 2 2 2 2 1 3] logD:  0.97\n"
     ]
    }
   ],
   "source": [
    "#@title Example minibatch data\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "print('Two example data points here:')\n",
    "print('-'* 50)\n",
    "print(f'{\"c1ccccc1NO\":<15} {\"logD: 1.48\":<25}')\n",
    "print(f'{\"Oc1ccccc1N\":<15} {\"logD: 0.97\":<25}')\n",
    "\n",
    "# Let's assume we have the dataset tokenized at the word level\n",
    "# using a dictonary.\n",
    "print('')\n",
    "print('The tokenized data shown below:')\n",
    "print('the minibatch of size = 2:')\n",
    "print('-'* 50)\n",
    "# '1' -> 1, 'c' -> 2, 'N' -> 3, 'O' -> 4\n",
    "tokenized_text = np.array([[2, 1, 2, 2, 2, 2, 2, 1, 3, 4],\n",
    "                           [4, 2, 1, 2, 2, 2, 2, 2, 1, 3]])\n",
    "logD = np.array([1.48, 0.97])\n",
    "\n",
    "# minibatch is the input to the lipo_net below.\n",
    "minibatch = {'inputs': tokenized_text, 'logD': logD}\n",
    "\n",
    "for i in range(minibatch['inputs'].shape[0]):\n",
    "  print('inputs: ', minibatch['inputs'][i], 'logD: ', minibatch['logD'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Lipophilicity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import relevant packages.\n",
    "# !pip install dm-haiku\n",
    "import jax\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## lipo_net with Transformer block. (with bugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants def.\n",
    "_VOCAB_SIZE = 20\n",
    "_EMBED_SIZE = 64\n",
    "_NUM_ATTENTION_HEAD = 4\n",
    "_NUM_TRANSFORMER_LAYERS = 1\n",
    "_DROPOUT_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lipo_net(batch):\n",
    "  \"\"\"The main network that predict logD from SMILES.\"\"\"\n",
    "  x = batch['inputs']  # (B, T), B:batch_size, T: sequence_length\n",
    "  seq_len = x.shape[-1]\n",
    "\n",
    "  # Embed the tokens.\n",
    "  embedder = hk.Embed(_VOCAB_SIZE, _EMBED_SIZE)\n",
    "  res = []\n",
    "  for i in range(x.shape[0]):\n",
    "    res.append(embedder(x[i]))\n",
    "  token_embed = jnp.stack(res)\n",
    "\n",
    "  trf_blocks = Transformer(\n",
    "      num_heads=_NUM_ATTENTION_HEAD,\n",
    "      num_layers=_NUM_TRANSFORMER_LAYERS,\n",
    "      dropout_rate=_DROPOUT_RATE)\n",
    "\n",
    "  out_tokens = trf_blocks(token_embed, is_training=True)\n",
    "\n",
    "  y = jnp.sum(out_tokens, axis=1)\n",
    "  return hk.Linear(1)(y)  # the predicted logD\n",
    "\n",
    "class Transformer(hk.Module):\n",
    "  \"\"\"A transformer stack.\"\"\"\n",
    "\n",
    "  def __init__(self, num_heads, num_layers, dropout_rate, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self._num_layers = num_layers\n",
    "    self._num_heads = num_heads\n",
    "    self._dropout_rate = dropout_rate\n",
    "\n",
    "  def __call__(self, h, is_training):\n",
    "    \"\"\"Connects the transformer.\n",
    "    Args:\n",
    "      h: Inputs, [B, T, D].\n",
    "      is_training: Whether we're training or not.\n",
    "    Returns:\n",
    "      Array of shape [B, T, D].\n",
    "    \"\"\"\n",
    "    print(f\"input embedding\")\n",
    "    print(\"-\"*50)\n",
    "    h_pooled = h.sum(axis=1)\n",
    "    print(h_pooled[0] - h_pooled[1])\n",
    "\n",
    "    init_scale = 2. / self._num_layers\n",
    "    dropout_rate = self._dropout_rate if is_training else 0.0\n",
    "    batch_size, seq_len, emb_size = h.shape\n",
    "\n",
    "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "    causal_mask = jnp.tile(causal_mask, (batch_size, 1, 1, 1))\n",
    "    # causal_mask if of shape (B, 1, T, T)\n",
    "    # the dummy 2nd dim of size 1 is for broadcasting over multi attention heads.\n",
    "\n",
    "    for i in range(self._num_layers):\n",
    "      # Pass through the multi-head attention.\n",
    "      # haiku.MultiHeadAttention\n",
    "      # https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/attention.py\n",
    "      attn_module = hk.MultiHeadAttention(num_heads=self._num_heads,\n",
    "                                          key_size=emb_size // self._num_heads,\n",
    "                                          model_size=emb_size,\n",
    "                                          w_init_scale=init_scale)\n",
    "      h_attn = attn_module(query=h, key=h, value=h) #, mask=causal_mask)\n",
    "      \n",
    "      print(f\"attention otuput layer {i}\")\n",
    "      print(\"-\"*50)\n",
    "      h_attn_pooled = h_attn.sum(axis=1)\n",
    "      print(h_attn_pooled[0] - h_attn_pooled[1])\n",
    "      \n",
    "      \n",
    "      h_attn = hk.dropout(hk.next_rng_key(), dropout_rate, h_attn)\n",
    "      h += h_attn\n",
    "      h = layer_norm(h, f\"atten_block_ln_{i}\")\n",
    "\n",
    "      # Pass through the MLP block for each token.\n",
    "      mlp_block = hk.Sequential([\n",
    "          hk.Linear(emb_size, w_init=hk.initializers.VarianceScaling(init_scale)),\n",
    "          jax.nn.gelu,\n",
    "          hk.Linear(emb_size, w_init=hk.initializers.VarianceScaling(init_scale))\n",
    "      ])\n",
    "      h_dense = mlp_block(h)\n",
    "      h_dense = hk.dropout(hk.next_rng_key(), dropout_rate, h_dense)\n",
    "      h += h_dense\n",
    "      h = layer_norm(h, f\"mlp_block_ln_{i}\")\n",
    "\n",
    "    # Apply LayerNorm at the end of the whole Transformer stack.\n",
    "    h = layer_norm(h, 'ln_final')\n",
    "    return h\n",
    "\n",
    "\n",
    "def layer_norm(x, name):\n",
    "  \"\"\"Apply a unique LayerNorm to x with default settings.\"\"\"\n",
    "  return hk.LayerNorm(\n",
    "      axis=-1, create_scale=True, create_offset=True, name=name)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embedding\n",
      "--------------------------------------------------\n",
      "[ 2.9802322e-08 -9.5367432e-07  0.0000000e+00 -2.3841858e-07\n",
      "  4.7683716e-07  0.0000000e+00 -4.7683716e-07  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -5.9604645e-08  0.0000000e+00\n",
      "  5.9604645e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -9.5367432e-07  2.3841858e-07  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -4.7683716e-07\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  4.7683716e-07  0.0000000e+00  0.0000000e+00  8.9406967e-08\n",
      " -2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  4.7683716e-07 -4.7683716e-07 -1.1920929e-07  0.0000000e+00\n",
      "  9.5367432e-07 -1.1920929e-07  0.0000000e+00  9.5367432e-07\n",
      "  0.0000000e+00  1.4901161e-08  0.0000000e+00  4.7683716e-07\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -4.7683716e-07  0.0000000e+00  9.5367432e-07 -4.7683716e-07\n",
      "  0.0000000e+00 -4.7683716e-07  1.1920929e-07  4.7683716e-07\n",
      " -4.7683716e-07  0.0000000e+00 -5.9604645e-07 -2.3841858e-07]\n",
      "attention otuput layer 0\n",
      "--------------------------------------------------\n",
      "[ 2.3841858e-07  2.3841858e-07 -2.3841858e-07  2.3841858e-07\n",
      " -1.7881393e-07  0.0000000e+00  1.7136335e-07  4.1723251e-07\n",
      " -4.7683716e-07  1.1920929e-07  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  2.9429793e-07 -4.7683716e-07  2.3841858e-07\n",
      "  4.7683716e-07  2.3841858e-07 -7.1525574e-07 -5.9604645e-07\n",
      "  4.7683716e-07 -1.1920929e-07 -5.9604645e-08  8.3446503e-07\n",
      "  8.6426735e-07 -4.7683716e-07  2.3841858e-07  1.1920929e-07\n",
      "  2.3841858e-07  0.0000000e+00  0.0000000e+00  2.3841858e-07\n",
      "  2.3841858e-07 -2.0861626e-07  4.7683716e-07 -2.2351742e-07\n",
      "  4.7683716e-07  1.7881393e-07 -9.5367432e-07 -2.3841858e-07\n",
      "  0.0000000e+00 -3.5762787e-07  1.4305115e-06 -1.7881393e-07\n",
      " -4.7683716e-07  0.0000000e+00 -2.3841858e-07 -4.7683716e-07\n",
      " -3.5762787e-07  2.3841858e-07  0.0000000e+00 -1.1920929e-07\n",
      "  5.9604645e-08  0.0000000e+00  0.0000000e+00 -2.3841858e-07\n",
      "  6.8545341e-07 -4.7683716e-07 -4.7683716e-07  2.3841858e-07\n",
      "  2.3841858e-07 -2.3841858e-07 -2.3841858e-07  4.7683716e-07]\n",
      "attention otuput layer 1\n",
      "--------------------------------------------------\n",
      "[-0.01444197  0.7202492  -0.59906197 -0.03962314 -0.08128667  0.14015198\n",
      " -0.35187054 -0.00324154  0.02645266 -0.02475119 -0.3012179   0.38400578\n",
      "  0.0509665   0.00985187 -0.1007148   0.05151296  0.70061445  0.16508448\n",
      "  0.3042711  -0.19579434  0.00194931  0.14439017  0.69067407  0.23783505\n",
      "  0.21854877  0.52664816  0.3181138   0.37142134 -0.14534491  0.24178576\n",
      " -0.21823251  0.06811762 -0.59174585  0.33523464  0.46799874 -0.19951344\n",
      " -0.2888837   0.0463109  -0.10619569  0.1781125   0.08070898  0.23633862\n",
      "  0.13288498 -0.12128967  0.34772313  0.15774909 -0.11514668 -0.2656821\n",
      "  0.3997661  -0.46146894  0.02295113  0.22721434 -0.11966312 -0.37002635\n",
      "  0.16306014 -0.63394773  0.05288029 -0.17460585  0.7373723  -0.18376309\n",
      "  0.18803453 -0.04715884  0.04974508 -0.83035016]\n",
      "attention otuput layer 2\n",
      "--------------------------------------------------\n",
      "[ 0.51184416 -0.49971247  1.0134025   0.13530993  0.24300814  0.3390255\n",
      "  0.09931183 -0.34033495  0.22376466 -0.6934705  -0.50191545  0.30176145\n",
      "  0.14137328  0.10601471 -0.1910119   1.0571566   0.33476412 -0.05903387\n",
      "  0.22918749 -0.5152178  -0.06536174 -0.21062422  0.255929   -0.04242325\n",
      " -0.15682459 -0.10630941 -0.21884966 -0.06461024  0.21885395 -0.22319126\n",
      "  0.36122847  0.6911292  -0.13286167  0.09485888  0.3348267   0.1292119\n",
      " -0.14073586 -0.33335304  0.07993793  0.68424237 -1.0592198   0.4359523\n",
      "  0.37449694  0.291528    0.17486     0.44280815  0.52174616  0.29289544\n",
      "  0.29458404 -0.65493536  0.06783152 -0.03010511  0.07181066  0.8293567\n",
      "  0.85323715 -0.06146121  0.16269946 -0.0074783   0.884078    0.09709644\n",
      " -0.42335916 -0.31117487 -0.21907675 -0.19179595]\n",
      "attention otuput layer 3\n",
      "--------------------------------------------------\n",
      "[ 0.55726147  0.6350031   1.4805883  -0.45984554 -0.5857613  -0.5706198\n",
      " -1.2895865  -0.18670958 -0.68578935 -0.42346668  0.23771524 -0.37172818\n",
      " -0.7877878   0.4490943  -0.39857888  0.2618375   0.23452902  1.1752882\n",
      "  0.24332829  0.62407255 -0.00441742 -0.09060764  0.00920677  0.6009302\n",
      "  0.6399477   0.2992748  -0.42692423  0.43933678  0.02552557 -0.22214901\n",
      " -1.2386643  -0.6313905  -0.7418105   0.38835144 -0.39349365  0.6030927\n",
      "  0.24127749 -0.54484034 -0.7367598  -0.7218547   0.49260283  0.31168747\n",
      " -0.11896753 -0.4539256   0.17133075  1.2800069  -0.2636093   0.2936592\n",
      " -0.32576036  0.24544649 -0.18449742  0.27289248 -0.71414113 -1.0364809\n",
      "  0.8093686   0.08701563  0.53954405  0.36690593  0.4257059   0.36591434\n",
      "  0.03399372 -0.08129555  0.67473793  0.40523887]\n",
      "input embedding\n",
      "--------------------------------------------------\n",
      "[ 2.9802322e-08 -9.5367432e-07  0.0000000e+00 -2.3841858e-07\n",
      "  4.7683716e-07  0.0000000e+00 -4.7683716e-07  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -5.9604645e-08  0.0000000e+00\n",
      "  5.9604645e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -9.5367432e-07  2.3841858e-07  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -4.7683716e-07\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  4.7683716e-07  0.0000000e+00  0.0000000e+00  8.9406967e-08\n",
      " -2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  4.7683716e-07 -4.7683716e-07 -1.1920929e-07  0.0000000e+00\n",
      "  9.5367432e-07 -1.1920929e-07  0.0000000e+00  9.5367432e-07\n",
      "  0.0000000e+00  1.4901161e-08  0.0000000e+00  4.7683716e-07\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -4.7683716e-07  0.0000000e+00  9.5367432e-07 -4.7683716e-07\n",
      "  0.0000000e+00 -4.7683716e-07  1.1920929e-07  4.7683716e-07\n",
      " -4.7683716e-07  0.0000000e+00 -5.9604645e-07 -2.3841858e-07]\n",
      "attention otuput layer 0\n",
      "--------------------------------------------------\n",
      "[ 2.3841858e-07  2.3841858e-07 -2.3841858e-07  2.3841858e-07\n",
      " -1.7881393e-07  0.0000000e+00  1.7136335e-07  4.1723251e-07\n",
      " -4.7683716e-07  1.1920929e-07  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  2.9429793e-07 -4.7683716e-07  2.3841858e-07\n",
      "  4.7683716e-07  2.3841858e-07 -7.1525574e-07 -5.9604645e-07\n",
      "  4.7683716e-07 -1.1920929e-07 -5.9604645e-08  8.3446503e-07\n",
      "  8.6426735e-07 -4.7683716e-07  2.3841858e-07  1.1920929e-07\n",
      "  2.3841858e-07  0.0000000e+00  0.0000000e+00  2.3841858e-07\n",
      "  2.3841858e-07 -2.0861626e-07  4.7683716e-07 -2.2351742e-07\n",
      "  4.7683716e-07  1.7881393e-07 -9.5367432e-07 -2.3841858e-07\n",
      "  0.0000000e+00 -3.5762787e-07  1.4305115e-06 -1.7881393e-07\n",
      " -4.7683716e-07  0.0000000e+00 -2.3841858e-07 -4.7683716e-07\n",
      " -3.5762787e-07  2.3841858e-07  0.0000000e+00 -1.1920929e-07\n",
      "  5.9604645e-08  0.0000000e+00  0.0000000e+00 -2.3841858e-07\n",
      "  6.8545341e-07 -4.7683716e-07 -4.7683716e-07  2.3841858e-07\n",
      "  2.3841858e-07 -2.3841858e-07 -2.3841858e-07  4.7683716e-07]\n",
      "attention otuput layer 1\n",
      "--------------------------------------------------\n",
      "[-0.37695742  0.47927284  0.31584167 -0.24929833  0.14941669 -0.2778821\n",
      " -0.16310978  0.27015162  0.05106401  0.4694134   0.12818971  0.05433798\n",
      " -0.20946765 -0.04947567  0.01383233 -0.17623329  0.4402814  -0.06888187\n",
      "  0.18173945  0.25098753  0.17000663  0.5277911   0.1590848   0.01518393\n",
      "  0.22312307  0.0594821  -0.06432295 -0.16174984 -0.04778659 -0.04794598\n",
      "  0.18929917 -0.21392798 -0.11106539  0.25454694 -0.02988029 -0.33363914\n",
      "  0.36702204 -0.19310665 -0.23198748  0.48554564  0.3347819   0.03560877\n",
      " -0.1678071  -0.31440663  0.13932943  0.1679888   0.05670099  0.27169633\n",
      "  0.15825021 -0.18927097 -0.19639039  0.18669426 -0.1729408   0.05650747\n",
      " -0.23426567 -0.1240927   0.22694111  0.19938421  0.24297217 -0.5575302\n",
      " -0.03409356 -0.0021168  -0.2023623  -0.25265336]\n",
      "attention otuput layer 2\n",
      "--------------------------------------------------\n",
      "[-0.27938128 -0.0332675  -0.42250347 -0.20968151  0.45861387  0.2798196\n",
      "  0.31653214 -0.08873826 -0.13878632  0.08313608 -0.24033928  0.1458863\n",
      " -0.58932614  0.28580692  0.108042   -0.3399427   0.25490868  0.62636757\n",
      " -0.02901411  0.16790795  0.8438337  -0.48066282  0.15017891 -0.10911238\n",
      " -0.15093845 -0.20922756 -0.31501055 -0.199224    0.29264927  1.0347626\n",
      "  0.88299555  0.18461156 -0.10077333 -0.00129008 -0.3694656  -0.22262907\n",
      "  0.1290369   0.5990162  -0.6174078  -0.02496338 -0.14735746 -0.33830428\n",
      " -0.32059646  0.161376    0.3556714  -0.5276592   0.01856732 -0.15642488\n",
      " -0.1044513   0.04575348  0.05922985  0.08455181  0.20160173  0.13987541\n",
      " -0.38886166 -0.14271665 -0.32326412  0.13431904 -0.43262315  0.07133245\n",
      " -0.06658173  0.39589977 -0.41151214 -0.01557529]\n",
      "attention otuput layer 3\n",
      "--------------------------------------------------\n",
      "[-2.5286651e-01 -5.0633907e-01  4.4008213e-01  2.7533245e-01\n",
      "  3.4563112e-01  2.9989290e-01  1.2506461e-01 -1.9194216e-01\n",
      "  2.3924661e-01  1.7405844e-01 -8.1169009e-01 -8.0659556e-01\n",
      " -3.2697508e-01  2.5294614e-01  1.8361855e-01 -4.2852223e-01\n",
      " -1.4854902e-01  9.2965126e-02  3.4269440e-01  8.3041191e-04\n",
      "  5.5117416e-01 -6.4019203e-02  5.9444237e-01 -1.7484665e-01\n",
      "  1.4131904e-01 -6.7407560e-01 -1.0497475e-01 -7.9373670e-01\n",
      "  2.5301147e-01  3.5003924e-01 -2.0311101e-01  1.5786946e-02\n",
      "  2.8669262e-01  7.3843241e-01  2.9037952e-01  5.4316616e-01\n",
      " -4.3987834e-01  9.8052120e-01 -2.8287137e-01 -1.0703516e+00\n",
      " -6.0521007e-01  4.1097021e-01 -9.7180653e-01  6.6702008e-01\n",
      "  6.0776865e-01  4.3577194e-02  9.7361088e-02 -9.4269276e-02\n",
      "  1.6191721e-02  2.4777043e-01 -1.8245554e-01 -2.6275384e-01\n",
      " -5.0818837e-01 -2.4018669e-01  4.2728496e-01  2.0769310e-01\n",
      "  8.0959004e-01  9.5386481e-01 -1.0542393e-01 -2.2750306e-01\n",
      " -5.9802723e-01 -5.8953965e-01  4.5659208e-01  6.9200373e-01]\n",
      "The predicted logD output from the lipo net:\n",
      "--------------------------------------------------\n",
      "[[1.5634933]\n",
      " [3.369591 ]]\n"
     ]
    }
   ],
   "source": [
    "# Demo of a single forward pass of the lipo_net.\n",
    "net = hk.transform(lipo_net)\n",
    "params = net.init(jax.random.PRNGKey(42), minibatch)\n",
    "model_out = net.apply(params, jax.random.PRNGKey(42), minibatch)\n",
    "print('The predicted logD output from the lipo net:')\n",
    "print('-'*50)\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Questions about the code.\n",
    "Let's assume we have a L2 loss function and a training loop to minimize the loss. <br>\n",
    "[Loss function and the training loop not shown for simplicity.] <br>\n",
    "Now we run the train loop and can see the loss has been minimzed. <br>\n",
    "\n",
    "We found the model performance is not great in general though. <br>\n",
    "Could you help debug by answering the questions below? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1. When evaluated, the model predicts the exact same logD for\n",
    "  *  Oc1ccccc1N\n",
    "  *  c1ccccc1NO  <br>\n",
    "These molecules are shown below. Can you think what ingredients may be missing from the model? <br>\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2. The model is not stable during training (output NaN)\n",
    "  * In your experience, what usually happens to the model so that it outputs Nan?\n",
    "  * What may be missing in our Trasnformer implementation that makes it less stable than a standard implementation?\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3. The model is not performing that well on the evaluation dataset.\n",
    " * From a hyperparameter sweep, strangely it seems the eval performance gets worse with higher dropout rate.\n",
    " * Could you guess what may be wrong in the training loop/model setup?\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4. We find the model to be slightly worse than the one from our colleagues.\n",
    "  * Which part of the implemented model may prevent the model's full capacity?\n",
    "  * Hint: Think of language models (e.g. GPT-x) compared to BERT\n",
    "What is the difference?\n",
    "\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 5. Can you find parts in the model that could be improved for performance?\n",
    "  * Any parts that can be vectorized/batch-applied?\n",
    "  * No need to be faimilar with Jax/Haiku. You can give pseudo-code in PyTorch/Tensorflow.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 6. Do you think this model will run for longer SMILES strings (e.g. 20 characters)?\n",
    "  * Yes or No? Can you explain why?\n",
    "  * Is there a specific line you may want to change to make it possibly generalize better for longer sequence?\n",
    "\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 7. Combining all the questions above, could you improve the model in the block below?\n",
    "  * Please try to keep the code backbone, and comment the changes you make.\n",
    "  * Feel free to use the syntax from PyTorch/Tensorflow with comments.\n",
    "  * We are not expecting the model to be syntax correct and runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
