{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    features: int # Output feature dimensionality for nodes\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, node_features, adj):\n",
    "        # node_features shape: (num_nodes, input_features)\n",
    "        # adj shape: (num_nodes, num_nodes)\n",
    "\n",
    "        # BUG 1: Flawed / Incomplete Graph Convolution\n",
    "        # This transformation only considers the node itself, ignoring neighbors.\n",
    "        # A proper GCN layer should aggregate features from neighbors using 'adj'.\n",
    "        # Example: `jnp.dot(adj, transformed_features)` would sum neighbor features.\n",
    "        transformed_features = nn.Dense(features=self.features)(node_features)\n",
    "\n",
    "        # This update rule doesn't actually use the graph structure (adj matrix)!\n",
    "        # It's essentially just applying an MLP to each node independently.\n",
    "        updated_features = nn.relu(transformed_features)\n",
    "\n",
    "        # Shape: (num_nodes, self.features)\n",
    "        return updated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, node_features, adj, training: bool):\n",
    "        # node_features: (batch_size, num_nodes, node_feature_dim)\n",
    "        # adj: (batch_size, num_nodes, num_nodes)\n",
    "        # Note: For simplicity, assuming all graphs in batch have same num_nodes (padding)\n",
    "\n",
    "        x = node_features\n",
    "        for _ in range(self.num_layers):\n",
    "            # Pass adjacency matrix, even though the buggy layer doesn't use it\n",
    "            x_new = GraphConvolution(features=self.hidden_dim)(x, adj)\n",
    "            x = x + x_new # Basic residual connection\n",
    "            x = nn.LayerNorm()(x)\n",
    "\n",
    "        # BUG 4: Graph Pooling / Readout - Summation Issue\n",
    "        # Summing node features can lead to embeddings that scale with graph size.\n",
    "        # Mean pooling (jnp.mean) is often more robust if graph sizes vary.\n",
    "        # graph_embedding = jnp.mean(x, axis=1) # axis=1 is the node dimension\n",
    "        graph_embedding = jnp.sum(x, axis=1) # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Prediction head for regression task\n",
    "        output = nn.Dense(features=1)(graph_embedding) # Predict a single continuous value\n",
    "        # Shape: (batch_size, 1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(prediction, target):\n",
    "    # prediction: (1,) - model output for one graph\n",
    "    # target: (1,) - true value for one graph\n",
    "    # This expects logits and binary labels (0 or 1).\n",
    "    # return optax.squared_error(prediction, target).squeeze() # Correct loss for regression\n",
    "    return optax.sigmoid_binary_cross_entropy(prediction, target.astype(jnp.int32)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(params, opt_state, batch, key, model_apply_fn):\n",
    "    node_features, adj, targets = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        predictions = model_apply_fn({'params': params}, node_features, adj, training=True, rngs={'dropout': key}) # Assuming dropout might be added later\n",
    "        # Ensure target is float for potential MSE loss later\n",
    "        loss = jnp.mean(calculate_loss(predictions, targets.astype(jnp.float32)))\n",
    "        return loss\n",
    "\n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_graph_batch(key, batch_size, num_nodes, node_feature_dim):\n",
    "    key, key_nodes, key_adj, key_targets = jax.random.split(key, 4)\n",
    "\n",
    "    # Random node features (e.g., atom properties)\n",
    "    node_features = jax.random.normal(key_nodes, (batch_size, num_nodes, node_feature_dim))\n",
    "\n",
    "    # Random adjacency matrix (simplified: symmetric, no self-loops for this example)\n",
    "    adj_dense = jax.random.randint(key_adj, (batch_size, num_nodes, num_nodes), 0, 2)\n",
    "    adj_symm = jnp.maximum(adj_dense, jnp.transpose(adj_dense, (0, 2, 1))) # Make symmetric\n",
    "    adj = jnp.where(jnp.eye(num_nodes, dtype=bool), 0, adj_symm) # Remove self-loops\n",
    "\n",
    "    # BUG 3: Target values potentially badly scaled\n",
    "    # Generating targets in a large range (e.g., 0-1000) when model outputs\n",
    "    # might initially be small (e.g., near 0) can lead to huge losses/gradients.\n",
    "    # Targets = jax.random.uniform(key_targets, (batch_size, 1), minval=0.0, maxval=10.0) # A more reasonable scale\n",
    "    targets = jax.random.uniform(key_targets, (batch_size, 1), minval=0.0, maxval=1000.0)\n",
    "\n",
    "    return (node_features, adj, targets), key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_FEATURE_DIM = 16\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 3\n",
    "NUM_NODES = 30 # Assume fixed size via padding for simplicity\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (8, 30, 16), (8, 30, 64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m dummy_nodes \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((BATCH_SIZE, NUM_NODES, NODE_FEATURE_DIM))\n\u001b[1;32m     10\u001b[0m dummy_adj \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((BATCH_SIZE, NUM_NODES, NUM_NODES))\n\u001b[0;32m---> 11\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_adj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize optimizer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mSimpleGNN.__call__\u001b[0;34m(self, node_features, adj, training)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Pass adjacency matrix, even though the buggy layer doesn't use it\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x_new \u001b[38;5;241m=\u001b[39m GraphConvolution(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)(x, adj)\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_new\u001b[49m \u001b[38;5;66;03m# Basic residual connection\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm()(x)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# BUG 4: Graph Pooling / Readout - Summation Issue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Summing node features can lead to embeddings that scale with graph size.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Mean pooling (jnp.mean) is often more robust if graph sizes vary.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# graph_embedding = jnp.mean(x, axis=1) # axis=1 is the node dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/test-3.10.14/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:579\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 579\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/test-3.10.14/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:180\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/test-3.10.14/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1215\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.add`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m  Array([10, 11, 12, 13], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_or(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/test-3.10.14/lib/python3.10/site-packages/jax/_src/lax/lax.py:135\u001b[0m, in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    133\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    136\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (8, 30, 16), (8, 30, 64)."
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "key = jax.random.PRNGKey(42)\n",
    "model_key, params_key, data_key, dropout_key, loop_key = jax.random.split(key, 5)\n",
    "\n",
    "# Instantiate model\n",
    "model = SimpleGNN(hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS)\n",
    "\n",
    "# Initialize parameters\n",
    "dummy_nodes = jnp.ones((BATCH_SIZE, NUM_NODES, NODE_FEATURE_DIM))\n",
    "dummy_adj = jnp.ones((BATCH_SIZE, NUM_NODES, NUM_NODES))\n",
    "params = model.init(params_key, dummy_nodes, dummy_adj, training=False)['params']\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optax.adam(learning_rate=LEARNING_RATE)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting GNN training...\")\n",
    "# Training loop\n",
    "for step in range(NUM_STEPS):\n",
    "    loop_key, data_key, dropout_key = jax.random.split(loop_key, 3)\n",
    "    batch, data_key = generate_dummy_graph_batch(data_key, BATCH_SIZE, NUM_NODES, NODE_FEATURE_DIM)\n",
    "\n",
    "    params, opt_state, loss = train_step(\n",
    "        params,\n",
    "        opt_state,\n",
    "        batch,\n",
    "        dropout_key, # Pass key even if dropout not used yet\n",
    "        model.apply\n",
    "    )\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
