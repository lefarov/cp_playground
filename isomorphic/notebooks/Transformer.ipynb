{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#This is a code reading comprehension practice.\n",
    "## Below there is a block of code below written in Jax/Haiku.<br>\n",
    "This code describes a [Transformer model](https://arxiv.org/abs/1706.03762). The model is intended to train to predict the lipophilicity (logD) directly from a SMILES string of a molecule. However there are errors in this code.<br>\n",
    "\n",
    "\n",
    "## We would like you to answer a few questions about the code.\n",
    "Please answer each questions in less than 5 sentences. <br>\n",
    "\n",
    "## We would also like you to try to give a corrected version of the model\n",
    "\n",
    "Note 1. There is no dataset and training loop. **You don't need to train the model.** <br>\n",
    "Note 2. The Jax/Haiku code reads very similar to PyTorch or Tensorflow. <br>\n",
    "Don't worry about the exact syntax when writing the corrected version of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task and dataset explanation:\n",
    "We have a simple artificial dataset where each data point is a pair of (SMILES, logD). In this artificial dataset you can assume all SMILES strings have 10 characters and that logD is a real number which is the regression target.\n",
    "\n",
    "You can find some example data in the code block below.\n",
    "Note that we assume the characters in SMILES are tokenized (str->List[int]) already.\n",
    "\n",
    "[**SMILES**](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) <br>\n",
    "The simplified molecular-input line-entry system (SMILES) is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings.\n",
    "\n",
    "For example, ethanol ($C_2H_6O$, 2D drawing shown below) can be written as **CCO** in the SMILES string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Example minibatch data\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "print('Two example data points here:')\n",
    "print('-'* 50)\n",
    "print(f'{\"c1ccccc1NO\":<15} {\"logD: 1.48\":<25}')\n",
    "print(f'{\"Oc1ccccc1N\":<15} {\"logD: 0.97\":<25}')\n",
    "\n",
    "# Let's assume we have the dataset tokenized at the word level\n",
    "# using a dictonary.\n",
    "print('')\n",
    "print('The tokenized data shown below:')\n",
    "print('the minibatch of size = 2:')\n",
    "print('-'* 50)\n",
    "# '1' -> 1, 'c' -> 2, 'N' -> 3, 'O' -> 4\n",
    "tokenized_text = np.array([[2, 1, 2, 2, 2, 2, 2, 1, 3, 4],\n",
    "                           [4, 2, 1, 2, 2, 2, 2, 2, 1, 3]])\n",
    "logD = np.array([1.48, 0.97])\n",
    "\n",
    "# minibatch is the input to the lipo_net below.\n",
    "minibatch = {'inputs': tokenized_text, 'logD': logD}\n",
    "\n",
    "for i in range(minibatch['inputs'].shape[0]):\n",
    "  print('inputs: ', minibatch['inputs'][i], 'logD: ', minibatch['logD'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Lipophilicity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import relevant packages.\n",
    "# !pip install dm-haiku\n",
    "import jax\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## lipo_net with Transformer block. (with bugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants def.\n",
    "_VOCAB_SIZE = 20\n",
    "_EMBED_SIZE = 64\n",
    "_NUM_ATTENTION_HEAD = 4\n",
    "_NUM_TRANSFORMER_LAYERS = 4\n",
    "_DROPOUT_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lipo_net(batch):\n",
    "  \"\"\"The main network that predict logD from SMILES.\"\"\"\n",
    "  x = batch['inputs']  # (B, T), B:batch_size, T: sequence_length\n",
    "  seq_len = x.shape[-1]\n",
    "\n",
    "  # Embed the tokens.\n",
    "  embedder = hk.Embed(_VOCAB_SIZE, _EMBED_SIZE)\n",
    "  res = []\n",
    "  for i in range(x.shape[0]):\n",
    "    res.append(embedder(x[i]))\n",
    "  token_embed = jnp.stack(res)\n",
    "\n",
    "  trf_blocks = Transformer(\n",
    "      num_heads=_NUM_ATTENTION_HEAD,\n",
    "      num_layers=_NUM_TRANSFORMER_LAYERS,\n",
    "      dropout_rate=_DROPOUT_RATE)\n",
    "\n",
    "  out_tokens = trf_blocks(token_embed, is_training=True)\n",
    "\n",
    "  y = jnp.sum(out_tokens, axis=1)\n",
    "  return hk.Linear(1)(y)  # the predicted logD\n",
    "\n",
    "class Transformer(hk.Module):\n",
    "  \"\"\"A transformer stack.\"\"\"\n",
    "\n",
    "  def __init__(self, num_heads, num_layers, dropout_rate, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self._num_layers = num_layers\n",
    "    self._num_heads = num_heads\n",
    "    self._dropout_rate = dropout_rate\n",
    "\n",
    "  def __call__(self, h, is_training):\n",
    "    \"\"\"Connects the transformer.\n",
    "    Args:\n",
    "      h: Inputs, [B, T, D].\n",
    "      is_training: Whether we're training or not.\n",
    "    Returns:\n",
    "      Array of shape [B, T, D].\n",
    "    \"\"\"\n",
    "\n",
    "    init_scale = 2. / self._num_layers\n",
    "    dropout_rate = self._dropout_rate\n",
    "    batch_size, seq_len, emb_size = h.shape\n",
    "\n",
    "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "    causal_mask = jnp.tile(causal_mask, (batch_size, 1, 1, 1))\n",
    "    # causal_mask if of shape (B, 1, T, T)\n",
    "    # the dummy 2nd dim of size 1 is for broadcasting over multi attention heads.\n",
    "\n",
    "    for i in range(self._num_layers):\n",
    "      # Pass through the multi-head attention.\n",
    "      # haiku.MultiHeadAttention\n",
    "      # https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/attention.py\n",
    "      attn_module = hk.MultiHeadAttention(num_heads=self._num_heads,\n",
    "                                          key_size=emb_size // self._num_heads,\n",
    "                                          model_size=emb_size,\n",
    "                                          w_init_scale=init_scale)\n",
    "      h_attn = attn_module(query=h, key=h, value=h, mask=causal_mask)\n",
    "      \n",
    "      h_attn = hk.dropout(hk.next_rng_key(), dropout_rate, h_attn)\n",
    "      h += h_attn\n",
    "\n",
    "      # Pass through the MLP block for each token.\n",
    "      mlp_block = hk.Sequential([\n",
    "          hk.Linear(emb_size, w_init=hk.initializers.VarianceScaling(init_scale)),\n",
    "          jax.nn.gelu,\n",
    "          hk.Linear(emb_size, w_init=hk.initializers.VarianceScaling(init_scale))\n",
    "      ])\n",
    "      h_dense = mlp_block(h)\n",
    "      h_dense = hk.dropout(hk.next_rng_key(), dropout_rate, h_dense)\n",
    "      h += h_dense\n",
    "\n",
    "    # Apply LayerNorm at the end of the whole Transformer stack.\n",
    "    h = layer_norm(h, 'ln_final')\n",
    "    return h\n",
    "\n",
    "\n",
    "def layer_norm(x, name):\n",
    "  \"\"\"Apply a unique LayerNorm to x with default settings.\"\"\"\n",
    "  return hk.LayerNorm(\n",
    "      axis=-1, create_scale=True, create_offset=True, name=name)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Demo of a single forward pass of the lipo_net.\n",
    "net = hk.transform(lipo_net)\n",
    "params = net.init(jax.random.PRNGKey(42), minibatch)\n",
    "model_out = net.apply(params, jax.random.PRNGKey(42), minibatch)\n",
    "print('The predicted logD output from the lipo net:')\n",
    "print('-'*50)\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Questions about the code.\n",
    "Let's assume we have a L2 loss function and a training loop to minimize the loss. <br>\n",
    "[Loss function and the training loop not shown for simplicity.] <br>\n",
    "Now we run the train loop and can see the loss has been minimzed. <br>\n",
    "\n",
    "We found the model performance is not great in general though. <br>\n",
    "Could you help debug by answering the questions below? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1. When evaluated, the model predicts the exact same logD for\n",
    "  *  Oc1ccccc1N\n",
    "  *  c1ccccc1NO  <br>\n",
    "These molecules are shown below. Can you think what ingredients may be missing from the model? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2. The model is not stable during training (output NaN)\n",
    "  * In your experience, what usually happens to the model so that it outputs Nan?\n",
    "  * What may be missing in our Trasnformer implementation that makes it less stable than a standard implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3. The model is not performing that well on the evaluation dataset.\n",
    " * From a hyperparameter sweep, strangely it seems the eval performance gets worse with higher dropout rate.\n",
    " * Could you guess what may be wrong in the training loop/model setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4. We find the model to be slightly worse than the one from our colleagues.\n",
    "  * Which part of the implemented model may prevent the model's full capacity?\n",
    "  * Hint: Think of language models (e.g. GPT-x) compared to BERT\n",
    "What is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 5. Can you find parts in the model that could be improved for performance?\n",
    "  * Any parts that can be vectorized/batch-applied?\n",
    "  * No need to be faimilar with Jax/Haiku. You can give pseudo-code in PyTorch/Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 6. Do you think this model will run for longer SMILES strings (e.g. 20 characters)?\n",
    "  * Yes or No? Can you explain why?\n",
    "  * Is there a specific line you may want to change to make it possibly generalize better for longer sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 7. Combining all the questions above, could you improve the model in the block below?\n",
    "  * Please try to keep the code backbone, and comment the changes you make.\n",
    "  * Feel free to use the syntax from PyTorch/Tensorflow with comments.\n",
    "  * We are not expecting the model to be syntax correct and runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
