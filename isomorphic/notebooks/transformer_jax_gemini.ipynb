{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProteinClassifier(nn.Module):\n",
    "    vocab_size: int  # Number of unique amino acids + padding token\n",
    "    embed_dim: int   # Dimension for embeddings\n",
    "    num_heads: int   # Number of attention heads\n",
    "    ff_dim: int      # Dimension of the feed-forward layer\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        # 1. Embed amino acid sequence\n",
    "        # Input shape: (batch_size, seq_len)\n",
    "        # Output shape: (batch_size, seq_len, embed_dim)\n",
    "        x = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)(x)\n",
    "\n",
    "        # BUG 1: Missing component for sequence order awareness\n",
    "\n",
    "        # 2. Simplified Transformer Block (Self-Attention + MLP)\n",
    "        # 2a. Multi-Head Self-Attention\n",
    "        attn_output = nn.SelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            qkv_features=self.embed_dim,\n",
    "            deterministic=not training, # Use dropout during training\n",
    "        )(x) # Input shape: (batch_size, seq_len, embed_dim)\n",
    "             # Output shape: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        x = x + attn_output # Residual connection\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # 2b. Feed-Forward Network\n",
    "        ff_output = nn.Sequential([\n",
    "            nn.Dense(features=self.ff_dim),\n",
    "            nn.relu,\n",
    "            nn.Dense(features=self.embed_dim) # Project back to embed_dim\n",
    "        ])(x)\n",
    "\n",
    "        x = x + ff_output # Residual connection\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # 3. Pooling & Classification Head\n",
    "        # Average pool across the sequence dimension\n",
    "        x = jnp.mean(x, axis=1) # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # Final dense layer for binary classification output (logits)\n",
    "        x = nn.Dense(features=1)(x) # Output shape: (batch_size, 1)\n",
    "        # Note: No sigmoid activation here, expecting loss function to handle logits\n",
    "\n",
    "        return x # Return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG 2: Inappropriate loss function for the task/output\n",
    "def calculate_loss(logits, labels):\n",
    "    # Expects one-hot encoded labels and multi-class logits\n",
    "    return optax.sigmoid_binary_cross_entropy(logits=logits.squeeze(), labels=labels)\n",
    "    # For binary tasks with single logit output, sigmoid_binary_cross_entropy is needed\n",
    "    # Also, the label format might be mismatched (e.g., expects float, gets int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(params, opt_state, batch, key, model_apply_fn, optimizer):\n",
    "    sequences, labels = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = model_apply_fn({'params': params}, sequences, training=True, rngs={'dropout': key})\n",
    "        loss = jnp.mean(calculate_loss(logits, labels.astype(jnp.float32))) # Ensure labels are float\n",
    "        return loss\n",
    "\n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, opt_state, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_batch(key, batch_size, seq_len, vocab_size):\n",
    "    key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "    # Generate random sequences\n",
    "    sequences = jax.random.randint(subkey1, (batch_size, seq_len), 0, vocab_size)\n",
    "\n",
    "    # BUG 3: Labels are completely random, no correlation with sequences\n",
    "    labels = jax.random.randint(subkey2, (batch_size, 1), 0, 2) # Random 0 or 1\n",
    "\n",
    "    return (sequences, labels), key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 22 # 20 amino acids + padding + mask token (example)\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 128\n",
    "SEQ_LEN = 50\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "model_key, params_key, data_key, dropout_key, loop_key = jax.random.split(key, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleProteinClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = jnp.ones((BATCH_SIZE, SEQ_LEN), dtype=jnp.int32)\n",
    "params = model.init(params_key, dummy_input, training=False)['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=LEARNING_RATE)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step: 0, Loss: 0.7017\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "# Training loop\n",
    "for step in range(NUM_STEPS):\n",
    "    loop_key, data_key, dropout_key = jax.random.split(loop_key, 3)\n",
    "    batch, data_key = generate_dummy_batch(data_key, BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n",
    "\n",
    "    params, opt_state, loss = train_step(\n",
    "        params,\n",
    "        opt_state,\n",
    "        batch,\n",
    "        dropout_key, # Pass dropout key\n",
    "        model.apply, # Pass model's apply function\n",
    "        optimizer\n",
    "    )\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(max_len, embed_dim):\n",
    "    \"\"\"Generates sinusoidal positional encoding matrix.\"\"\"\n",
    "    position = jnp.arange(max_len)[:, jnp.newaxis] # (max_len, 1)\n",
    "    div_term = jnp.exp(jnp.arange(0, embed_dim, 2) * -(jnp.log(10000.0) / embed_dim)) # (embed_dim/2,)\n",
    "    pe = jnp.zeros((max_len, embed_dim))\n",
    "\n",
    "    pe = pe.at[:, 0::2].set(jnp.sin(position * div_term))\n",
    "    pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))\n",
    "\n",
    "    # Add batch dimension: (1, max_len, embed_dim)\n",
    "    pe = pe[jnp.newaxis, :, :]\n",
    "    return pe # shape (1, max_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoidal_positional_encoding(SEQ_LEN, EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
