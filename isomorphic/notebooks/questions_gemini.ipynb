{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231586c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax # Commonly used for loss functions like cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d20d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, targets, padding_value=0):\n",
    "  \"\"\"\n",
    "  Calculates the cross-entropy loss for sequence prediction.\n",
    "\n",
    "  Args:\n",
    "    logits: Predicted logits from the model. Shape: (batch_size, seq_len, num_classes)\n",
    "    targets: Ground truth target labels (integers). Shape: (batch_size, seq_len)\n",
    "    padding_value: The integer value used for padding in the targets.\n",
    "\n",
    "  Returns:\n",
    "    The mean loss calculated over the batch.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate cross-entropy loss for each position\n",
    "  # optax.softmax_cross_entropy_with_integer_labels expects logits of shape (..., num_classes)\n",
    "  # and labels of shape (...)\n",
    "  # It returns loss of shape (...)\n",
    "  mask = targets == -1\n",
    "  \n",
    "  token_loss = optax.softmax_cross_entropy_with_integer_labels(logits[~mask], targets[~mask])\n",
    "  # Shape: (batch_size, seq_len)\n",
    "\n",
    "  # Calculate the mean loss across all tokens in the batch\n",
    "  mean_loss = jnp.mean(token_loss)\n",
    "\n",
    "  return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a78923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Loss: 2.4950876235961914\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "batch_size = 4\n",
    "seq_len = 50\n",
    "num_classes = 8 # e.g., 8 secondary structure types\n",
    "\n",
    "# Dummy model output (logits)\n",
    "dummy_logits = jax.random.normal(key, (batch_size, seq_len, num_classes))\n",
    "\n",
    "# Dummy targets with padding (using 0 as padding token)\n",
    "# Let's assume sequence lengths are [40, 30, 50, 20]\n",
    "dummy_targets = jax.random.randint(key, (batch_size, seq_len), 1, num_classes) # Values from 1 to num_classes\n",
    "dummy_targets = dummy_targets.at[0, 40:].set(-1) # Pad first sequence\n",
    "dummy_targets = dummy_targets.at[1, 30:].set(-1) # Pad second sequence\n",
    "# Third sequence is full length (50)\n",
    "dummy_targets = dummy_targets.at[3, 20:].set(-1) # Pad fourth sequence\n",
    "\n",
    "# Calculate loss\n",
    "loss = calculate_loss(dummy_logits, dummy_targets, padding_value=0)\n",
    "\n",
    "print(f\"Calculated Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "713aa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "from typing import Sequence, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f5e4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_init(max_len=2048, embed_dim=512):\n",
    "    position = np.arange(max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "    pe = np.zeros((max_len, embed_dim))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return jnp.array(pe)[np.newaxis, :, :] # Add batch dim: (1, max_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8d81668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.        ,  1.        ,  0.        ,  1.        ],\n",
       "        [ 0.84147096,  0.5403023 ,  0.00999983,  0.99995   ],\n",
       "        [ 0.9092974 , -0.41614684,  0.01999867,  0.9998    ],\n",
       "        [ 0.14112   , -0.9899925 ,  0.0299955 ,  0.99955004],\n",
       "        [-0.7568025 , -0.6536436 ,  0.03998933,  0.9992001 ],\n",
       "        [-0.9589243 ,  0.2836622 ,  0.04997917,  0.99875027],\n",
       "        [-0.2794155 ,  0.96017027,  0.059964  ,  0.99820054],\n",
       "        [ 0.6569866 ,  0.75390226,  0.06994285,  0.997551  ],\n",
       "        [ 0.98935825, -0.14550003,  0.0799147 ,  0.99680173],\n",
       "        [ 0.4121185 , -0.91113025,  0.08987855,  0.9959527 ]]],      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoidal_init(10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4625bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    embed_dim: int\n",
    "    max_len: int = 2048 # Max sequence length anticipate\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, seq_len: int):\n",
    "        # Returns PE for the actual seq_len: (1, seq_len, embed_dim)\n",
    "        full_pe = self.param('pe', lambda: sinusoidal_init(self.max_len, self.embed_dim))\n",
    "        # Ensure requires_grad is False if using self.param for fixed PE\n",
    "        # Or just compute directly if truly fixed:\n",
    "        # full_pe = sinusoidal_init(self.max_len, self.embed_dim)\n",
    "        return jax.lax.dynamic_slice_in_dim(full_pe, 0, seq_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "749ddf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  hidden_dim: int\n",
    "  output_dim: int\n",
    "  dropout_rate: float = 0.1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, *, deterministic: bool):\n",
    "    x = nn.Dense(features=self.hidden_dim)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = nn.Dense(features=self.output_dim)(x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bad35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  embed_dim: int\n",
    "  num_heads: int\n",
    "  mlp_dim: int\n",
    "  dropout_rate: float = 0.1\n",
    "  max_len: int = 2048 # Needed for PE inside\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, mask: Optional[jnp.ndarray] = None, *, deterministic: bool):\n",
    "    seq_len = x.shape[1]\n",
    "    pos_encoding_layer = PositionalEncoding(embed_dim=self.embed_dim, max_len=self.max_len)\n",
    "    # Apply positional encoding *inside* the block\n",
    "    x = x + pos_encoding_layer(seq_len) # Add PE to input 'x'\n",
    "\n",
    "    attn_output = nn.SelfAttention(\n",
    "        num_heads=self.num_heads,\n",
    "        qkv_features=self.embed_dim,\n",
    "        dropout_rate=self.dropout_rate,\n",
    "        deterministic=deterministic\n",
    "    )(inputs_q=x, mask=mask)\n",
    "\n",
    "    x_attn = x + attn_output\n",
    "    x_attn_norm = nn.LayerNorm()(x_attn)\n",
    "\n",
    "    mlp_output = MLPBlock(\n",
    "        hidden_dim=self.mlp_dim,\n",
    "        output_dim=self.embed_dim,\n",
    "        dropout_rate=self.dropout_rate\n",
    "    )(x_attn_norm, deterministic=deterministic)\n",
    "\n",
    "    x_mlp = x_attn_norm + mlp_output\n",
    "    output = nn.LayerNorm()(x_mlp)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8101524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    num_layers: int\n",
    "    vocab_size: int # e.g., 21 for amino acids + padding\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    mlp_dim: int\n",
    "    max_len: int = 2048\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x_tokens, padding_mask: Optional[jnp.ndarray] = None, *, train: bool):\n",
    "        # 1. Input Embedding\n",
    "        embed_layer = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)\n",
    "        x = embed_layer(x_tokens) # Shape: (batch, seq_len, embed_dim)\n",
    "\n",
    "        # Apply embedding dropout\n",
    "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=not train)\n",
    "\n",
    "        # 2. Encoder Stack\n",
    "        for _ in range(self.num_layers):\n",
    "             # Note: This EncoderBlock internally adds PE\n",
    "             block = EncoderBlock(embed_dim=self.embed_dim,\n",
    "                                  num_heads=self.num_heads,\n",
    "                                  mlp_dim=self.mlp_dim,\n",
    "                                  dropout_rate=self.dropout_rate,\n",
    "                                  max_len=self.max_len)\n",
    "             x = block(x, mask=padding_mask, deterministic=not train) # `mask` needs correct format for SelfAttention\n",
    "\n",
    "        # 3. Final LayerNorm (Optional, sometimes applied)\n",
    "        # x = nn.LayerNorm()(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d213ca99",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PositionalEncoding.__call__.<locals>.<lambda>() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m dummy_tokens \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(key, (batch_size, seq_len), \u001b[38;5;241m0\u001b[39m, vocab_size)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize and apply (training mode)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use train=False for init determinism\u001b[39;00m\n\u001b[1;32m     18\u001b[0m output_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, dummy_tokens, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Use train=True for apply\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tokens shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_tokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m, in \u001b[0;36mTransformerEncoder.__call__\u001b[0;34m(self, x_tokens, padding_mask, train)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     21\u001b[0m      \u001b[38;5;66;03m# Note: This EncoderBlock internally adds PE\u001b[39;00m\n\u001b[1;32m     22\u001b[0m      block \u001b[38;5;241m=\u001b[39m EncoderBlock(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m     23\u001b[0m                           num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m     24\u001b[0m                           mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_dim,\n\u001b[1;32m     25\u001b[0m                           dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate,\n\u001b[1;32m     26\u001b[0m                           max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len)\n\u001b[0;32m---> 27\u001b[0m      x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# `mask` needs correct format for SelfAttention\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 3. Final LayerNorm (Optional, sometimes applied)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# x = nn.LayerNorm()(x)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mEncoderBlock.__call__\u001b[0;34m(self, x, mask, deterministic)\u001b[0m\n\u001b[1;32m     11\u001b[0m pos_encoding_layer \u001b[38;5;241m=\u001b[39m PositionalEncoding(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply positional encoding *inside* the block\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mpos_encoding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add PE to input 'x'\u001b[39;00m\n\u001b[1;32m     15\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m     16\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m     17\u001b[0m     qkv_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m     18\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate,\n\u001b[1;32m     19\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic\n\u001b[1;32m     20\u001b[0m )(inputs_q\u001b[38;5;241m=\u001b[39mx, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m     22\u001b[0m x_attn \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_output\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m, in \u001b[0;36mPositionalEncoding.__call__\u001b[0;34m(self, seq_len)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq_len: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Returns PE for the actual seq_len: (1, seq_len, embed_dim)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     full_pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msinusoidal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Ensure requires_grad is False if using self.param for fixed PE\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Or just compute directly if truly fixed:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# full_pe = sinusoidal_init(self.max_len, self.embed_dim)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mdynamic_slice_in_dim(full_pe, \u001b[38;5;241m0\u001b[39m, seq_len, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/test-3.10.14/lib/python3.10/site-packages/flax/core/scope.py:968\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    967\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[0;32m--> 968\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, value)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unbox:\n",
      "\u001b[0;31mTypeError\u001b[0m: PositionalEncoding.__call__.<locals>.<lambda>() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(2)\n",
    "batch_size = 2\n",
    "seq_len = 50\n",
    "vocab_size = 21\n",
    "embed_dim = 64\n",
    "num_heads = 2\n",
    "mlp_dim = 128\n",
    "num_layers = 3\n",
    "\n",
    "model = TransformerEncoder(num_layers=num_layers, vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                           num_heads=num_heads, mlp_dim=mlp_dim)\n",
    "\n",
    "# Dummy input tokens (integers)\n",
    "dummy_tokens = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)\n",
    "\n",
    "# Initialize and apply (training mode)\n",
    "variables = model.init(key, dummy_tokens, train=False) # Use train=False for init determinism\n",
    "output_embeddings = model.apply(variables, dummy_tokens, train=True) # Use train=True for apply\n",
    "\n",
    "print(f\"Input tokens shape: {dummy_tokens.shape}\")\n",
    "print(f\"Output embeddings shape: {output_embeddings.shape}\")\n",
    "\n",
    "# The permutation invariance issue would be tested by actually shuffling\n",
    "# dummy_tokens along the seq_len dimension and observing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9ccb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "# Using jax.ops for segment_sum is common, but let's use the newer jax.ops.segment_sum\n",
    "# which is equivalent to jax.ops.index_add with x=data at indices=indices\n",
    "# For clarity, we'll use the explicit jax.ops.segment_sum style if needed,\n",
    "# or illustrate with index operations directly. Let's use segment_sum.\n",
    "# Note: As of newer JAX versions, prefer scatter operations.\n",
    "# Let's use the scatter approach for modernity.\n",
    "# from jax.ops import segment_sum # Older style\n",
    "from jax import ops # For scatter add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea033c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNNLayer(nn.Module):\n",
    "    out_dim: int\n",
    "    activation: callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, node_features: jnp.ndarray, senders: jnp.ndarray, receivers: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        Applies a simple GNN layer.\n",
    "\n",
    "        Args:\n",
    "          node_features: Node features. Shape: (num_nodes, in_dim)\n",
    "          senders: Sender indices for each edge. Shape: (num_edges,)\n",
    "          receivers: Receiver indices for each edge. Shape: (num_edges,)\n",
    "\n",
    "        Returns:\n",
    "          Updated node features. Shape: (num_nodes, out_dim)\n",
    "        \"\"\"\n",
    "        num_nodes = node_features.shape[0]\n",
    "        in_dim = node_features.shape[-1]\n",
    "\n",
    "        # 1. Transform node features (linear layer for messages)\n",
    "        message_features = nn.Dense(features=self.out_dim)(node_features)\n",
    "\n",
    "        # 2. Gather features from sending nodes for each edge\n",
    "        sender_features = message_features[senders] # Shape: (num_edges, out_dim)\n",
    "\n",
    "        # 3. Aggregate messages for each receiving node using sum\n",
    "        # We sum all incoming messages for each node 'i'.\n",
    "        # Use jax.ops.scatter_add: adds values into updates at indices.\n",
    "        # Initialize aggregated messages to zeros\n",
    "        aggregated_messages = jnp.zeros((num_nodes, self.out_dim), dtype=sender_features.dtype)\n",
    "        # Add features from senders to their corresponding receivers\n",
    "        # For each edge (sender -> receiver), add sender_feature to aggregated_messages[receiver]\n",
    "        aggregated_messages = aggregated_messages.at[receivers].add(sender_features)\n",
    "        # Alternative using older segment_sum:\n",
    "        # aggregated_messages = ops.segment_sum(sender_features, receivers, num_segments=num_nodes)\n",
    "\n",
    "\n",
    "        # 4. Update node representation using aggregated messages\n",
    "        # (Note: A common update also includes the node's previous features, e.g., via sum or concat,\n",
    "        # but this simplified version uses only aggregated messages for the update basis)\n",
    "        new_node_features = self.activation(aggregated_messages)\n",
    "\n",
    "        return new_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "957046c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 10\n",
      "Total edges: 50\n",
      "Node in-degrees: [26.  0.  3.  4.  3.  3.  2.  2.  2.  5.]\n",
      "Max in-degree: 26.0\n",
      "Input features shape: (10, 16)\n",
      "Output features shape: (10, 32)\n",
      "Sample output features (node 0): [ 2.2878036 19.458845   3.3662817  0.        18.386833 ]...\n",
      "Output feature norm (node 0): 45.07743453979492\n",
      "Output feature norm (node 1): 0.0\n"
     ]
    }
   ],
   "source": [
    "# --- Example Graph Data & Usage ---\n",
    "key = jax.random.PRNGKey(4)\n",
    "num_nodes = 10\n",
    "num_edges = 30\n",
    "in_dim = 16\n",
    "out_dim = 32\n",
    "\n",
    "# Dummy node features\n",
    "node_features = jax.random.normal(key, (num_nodes, in_dim))\n",
    "\n",
    "# Dummy graph structure (edge list)\n",
    "# Ensure some nodes have high degree, others low degree\n",
    "senders = jax.random.randint(key, (num_edges,), 0, num_nodes)\n",
    "receivers = jax.random.randint(key, (num_edges,), 0, num_nodes)\n",
    "\n",
    "# Make node 0 a high-degree node (receives many messages)\n",
    "high_degree_node_idx = 0\n",
    "num_extra_edges = 20\n",
    "extra_senders = jax.random.randint(key, (num_extra_edges,), 1, num_nodes) # Avoid self-loops for simplicity\n",
    "extra_receivers = jnp.full((num_extra_edges,), high_degree_node_idx)\n",
    "\n",
    "senders = jnp.concatenate([senders, extra_senders])\n",
    "receivers = jnp.concatenate([receivers, extra_receivers])\n",
    "num_edges = senders.shape[0]\n",
    "\n",
    "print(f\"Total nodes: {num_nodes}\")\n",
    "print(f\"Total edges: {num_edges}\")\n",
    "# Calculate in-degrees to show variance\n",
    "in_degrees = jnp.zeros((num_nodes,)).at[receivers].add(1)\n",
    "print(f\"Node in-degrees: {in_degrees}\")\n",
    "print(f\"Max in-degree: {jnp.max(in_degrees)}\")\n",
    "\n",
    "# Initialize and apply the GNN layer\n",
    "gnn_layer = SimpleGNNLayer(out_dim=out_dim)\n",
    "variables = gnn_layer.init(key, node_features, senders, receivers)\n",
    "\n",
    "# Forward pass\n",
    "# Wrap in a simple function to simulate potential gradient calculation context\n",
    "@jax.jit\n",
    "def apply_layer(params, features, senders, receivers):\n",
    "    return gnn_layer.apply({'params': params}, features, senders, receivers)\n",
    "\n",
    "output_features = apply_layer(variables['params'], node_features, senders, receivers)\n",
    "\n",
    "print(f\"Input features shape: {node_features.shape}\")\n",
    "print(f\"Output features shape: {output_features.shape}\")\n",
    "print(f\"Sample output features (node 0): {output_features[0, :5]}...\")\n",
    "print(f\"Output feature norm (node 0): {jnp.linalg.norm(output_features[0])}\")\n",
    "print(f\"Output feature norm (node 1): {jnp.linalg.norm(output_features[1])}\") # Likely lower degree\n",
    "\n",
    "# In a real training loop, large norms here could lead to NaNs after loss/gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f54728",
   "metadata": {},
   "outputs": [
    {
     "ename": "TransformTargetError",
     "evalue": "Linen transformations must be applied to Modules classes or functions taking a Module instance as the first argument. The provided target is not a Module class or callable: LSTMCell(\n    # attributes\n    features = 64\n    gate_fn = sigmoid\n    activation_fn = tanh\n    kernel_init = init\n    recurrent_kernel_init = init\n    bias_init = zeros\n    dtype = None\n    param_dtype = float32\n    carry_init = zeros\n) (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.TransformTargetError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTransformTargetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 82\u001b[0m\n\u001b[1;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleRNNClassifier(\n\u001b[1;32m     75\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[1;32m     76\u001b[0m     rnn_hidden_size\u001b[38;5;241m=\u001b[39mrnn_hidden_size,\n\u001b[1;32m     77\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m     78\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mvocab_size\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Initialize model parameters\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     85\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, dummy_inputs, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mSimpleRNNClassifier.__call__\u001b[0;34m(self, inputs, train)\u001b[0m\n\u001b[1;32m     34\u001b[0m initial_carry \u001b[38;5;241m=\u001b[39m rnn_cell\u001b[38;5;241m.\u001b[39minitialize_carry(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m), (batch_size,))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Scan RNN cell over sequence\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Use nn.scan for efficient processing over the sequence length axis\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m scan_rnn \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnn_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_rngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Scan over seq_length\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m final_carry, outputs \u001b[38;5;241m=\u001b[39m scan_rnn(initial_carry, embedded_inputs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# outputs shape: (batch_size, seq_length, rnn_hidden_size)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# final_carry is a tuple (hidden_state, cell_state)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# final_hidden_state shape: (batch_size, rnn_hidden_size)\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/test-3.10.14/lib/python3.10/site-packages/flax/linen/transforms.py:786\u001b[0m, in \u001b[0;36mlift_transform\u001b[0;34m(transform, target, methods, *trafo_args, **trafo_kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m decorator_lift_transform(\n\u001b[1;32m    783\u001b[0m     transform, target, \u001b[38;5;241m*\u001b[39mtrafo_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrafo_kwargs\n\u001b[1;32m    784\u001b[0m   )\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 786\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mTransformTargetError(target)\n",
      "\u001b[0;31mTransformTargetError\u001b[0m: Linen transformations must be applied to Modules classes or functions taking a Module instance as the first argument. The provided target is not a Module class or callable: LSTMCell(\n    # attributes\n    features = 64\n    gate_fn = sigmoid\n    activation_fn = tanh\n    kernel_init = init\n    recurrent_kernel_init = init\n    bias_init = zeros\n    dtype = None\n    param_dtype = float32\n    carry_init = zeros\n) (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.TransformTargetError)"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from typing import Sequence\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    \"\"\"A simple RNN for sequence classification.\"\"\"\n",
    "    num_classes: int\n",
    "    rnn_hidden_size: int\n",
    "    embedding_dim: int\n",
    "    vocab_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray, train: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Batch of input token sequences, shape (batch_size, seq_length).\n",
    "            train: Boolean indicating if the model is in training mode.\n",
    "\n",
    "        Returns:\n",
    "            Output logits for classification, shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # 1. Embed the input tokens\n",
    "        embed = nn.Embed(num_embeddings=self.vocab_size, features=self.embedding_dim)\n",
    "        embedded_inputs = embed(inputs)\n",
    "        # embedded_inputs shape: (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        # 2. Process sequence with RNN\n",
    "        # Initialize RNN cell\n",
    "        rnn_cell = nn.LSTMCell(features=self.rnn_hidden_size, name=\"lstm_cell\")\n",
    "\n",
    "        # Initialize hidden state\n",
    "        batch_size = inputs.shape[0]\n",
    "        initial_carry = rnn_cell.initialize_carry(jax.random.PRNGKey(0), (batch_size,))\n",
    "\n",
    "        # Scan RNN cell over sequence\n",
    "        # Use nn.scan for efficient processing over the sequence length axis\n",
    "        scan_rnn = nn.scan(\n",
    "            rnn_cell,\n",
    "            variable_broadcast=\"params\",\n",
    "            split_rngs={\"params\": False},\n",
    "            in_axes=1,  # Scan over seq_length\n",
    "            out_axes=1\n",
    "        )\n",
    "        final_carry, outputs = scan_rnn(initial_carry, embedded_inputs)\n",
    "        # outputs shape: (batch_size, seq_length, rnn_hidden_size)\n",
    "        # final_carry is a tuple (hidden_state, cell_state)\n",
    "        # final_hidden_state shape: (batch_size, rnn_hidden_size)\n",
    "\n",
    "        final_hidden_state = final_carry[0] # Use the final hidden state for classification\n",
    "\n",
    "        # 3. Classification Head\n",
    "        # Dense layer to map final hidden state to number of classes\n",
    "        output_logits = nn.Dense(features=self.num_classes, name=\"output_dense\")(final_hidden_state)\n",
    "        # output_logits shape: (batch_size, num_classes)\n",
    "\n",
    "        # Apply activation function\n",
    "        output_probs = nn.relu(output_logits)\n",
    "\n",
    "        return output_probs # Should represent class scores/probabilities\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume some dummy data\n",
    "batch_size = 4\n",
    "seq_length = 10\n",
    "vocab_size = 100\n",
    "embedding_dim = 32\n",
    "rnn_hidden_size = 64\n",
    "num_classes = 5\n",
    "\n",
    "key = jax.random.PRNGKey(1)\n",
    "dummy_inputs = jax.random.randint(key, (batch_size, seq_length), 0, vocab_size)\n",
    "\n",
    "model = SimpleRNNClassifier(\n",
    "    num_classes=num_classes,\n",
    "    rnn_hidden_size=rnn_hidden_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Initialize model parameters\n",
    "variables = model.init(key, dummy_inputs, train=False)\n",
    "\n",
    "# Forward pass\n",
    "output = model.apply(variables, dummy_inputs, train=False)\n",
    "\n",
    "print(\"Input shape:\", dummy_inputs.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "# Expected output shape: (batch_size, num_classes)\n",
    "# Output values should be suitable for cross-entropy loss\n",
    "\n",
    "# --- Training Loss (Conceptual) ---\n",
    "# Assume 'dummy_labels' are integer class indices, shape (batch_size,)\n",
    "# dummy_labels = jnp.array([1, 0, 4, 2])\n",
    "\n",
    "# def cross_entropy_loss(outputs, labels):\n",
    "#     one_hot_labels = jax.nn.one_hot(labels, num_classes=num_classes)\n",
    "#     # Standard cross-entropy often expects logits or log-probabilities\n",
    "#     # For instance, optax.softmax_cross_entropy expects logits\n",
    "#     # loss = -jnp.sum(one_hot_labels * jnp.log(outputs + 1e-7), axis=-1) # Manual CE assuming probabilities\n",
    "#     # Or using a library function (e.g., from optax)\n",
    "#     # import optax\n",
    "#     # loss = optax.softmax_cross_entropy(logits=???, labels=one_hot_labels)\n",
    "#     return jnp.mean(loss)\n",
    "\n",
    "# # Problem: Why might training using standard cross-entropy loss on 'output' fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090eb6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
